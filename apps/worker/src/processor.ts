/**
 * RecoverHub Worker â€” Job Processors
 *
 * Job types:
 *   1. "batch-scan"    â€” Fetches due retry attempts and enqueues individual jobs
 *   2. "retry-job"     â€” Executes a single retry attempt
 *   3. "dunning-scan"  â€” Fetches due dunning emails and enqueues individual jobs
 *   4. "dunning-email" â€” Sends a single dunning email via Resend
 */

import { Worker, Job } from "bullmq";
import { fetchDueRetries, executeRetryAttempt } from "./retry";
import { fetchDueDunningEmails, executeDunningEmail } from "./dunning";
import {
  connection,
  retryQueue,
  dunningQueue,
  RETRY_QUEUE_NAME,
  DUNNING_QUEUE_NAME,
  type RetryJobData,
  type BatchScanJobData,
  type DunningEmailJobData,
  type DunningScanJobData,
} from "./queue";

// â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const DEFAULT_BATCH_SIZE = 50;
const CONCURRENCY = 5;

// â”€â”€â”€ Batch Scan Processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function processBatchScan(job: Job<BatchScanJobData>): Promise<void> {
  const batchSize = job.data.batchSize ?? DEFAULT_BATCH_SIZE;

  console.log(
    `[batch-scan] ğŸ” Starting scan (triggeredAt=${job.data.triggeredAt}, batchSize=${batchSize})`
  );

  await job.updateProgress(10);

  const dueRetries = await fetchDueRetries(batchSize);

  console.log(`[batch-scan] Found ${dueRetries.length} due retry attempt(s)`);

  if (dueRetries.length === 0) {
    console.log("[batch-scan] âœ… Nothing to process");
    return;
  }

  await job.updateProgress(40);

  // Enqueue one retry-job per due attempt
  const jobPayloads = dueRetries.map((r) => ({
    name: "retry-job",
    data: {
      failedPaymentId: r.failedPaymentId,
      attemptId: r.attemptId,
      attemptNumber: r.attemptNumber,
    } satisfies RetryJobData,
  }));

  await retryQueue.addBulk(jobPayloads);

  await job.updateProgress(100);

  console.log(`[batch-scan] âœ… Enqueued ${dueRetries.length} retry job(s)`);
}

// â”€â”€â”€ Retry Job Processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function processRetryJob(job: Job<RetryJobData>): Promise<void> {
  const { failedPaymentId, attemptId, attemptNumber } = job.data;

  console.log(
    `[retry-job] âš¡ Attempt #${attemptNumber} for payment ${failedPaymentId}`
  );

  await job.updateProgress(20);

  const result = await executeRetryAttempt(failedPaymentId, attemptId);

  await job.updateProgress(100);

  if (result.success) {
    console.log(`[retry-job] âœ… Recovered: ${failedPaymentId}`);
  } else {
    const nextInfo = result.nextRetryAt
      ? ` | next: ${result.nextRetryAt.toISOString()}`
      : " | paused (all attempts exhausted)";
    console.log(`[retry-job] âŒ Attempt #${attemptNumber} failed: ${result.error}${nextInfo}`);
  }
}

// â”€â”€â”€ Dunning Scan Processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function processDunningScan(job: Job<DunningScanJobData>): Promise<void> {
  const batchSize = job.data.batchSize ?? DEFAULT_BATCH_SIZE;

  console.log(
    `[dunning-scan] ğŸ” Starting scan (triggeredAt=${job.data.triggeredAt}, batchSize=${batchSize})`
  );

  await job.updateProgress(10);

  const dueEmails = await fetchDueDunningEmails(batchSize);

  console.log(`[dunning-scan] Found ${dueEmails.length} due dunning email(s)`);

  if (dueEmails.length === 0) {
    console.log("[dunning-scan] âœ… Nothing to send");
    return;
  }

  await job.updateProgress(40);

  // Enqueue one dunning-email job per due email
  const jobPayloads = dueEmails.map((d) => ({
    name: "dunning-email",
    data: {
      failedPaymentId: d.failedPaymentId,
      templateId: d.templateId,
      sequenceOrder: d.sequenceOrder,
    } satisfies DunningEmailJobData,
    opts: {
      // Deduplicate: one send per payment+template
      jobId: `dunning:${d.failedPaymentId}:seq${d.sequenceOrder}`,
    },
  }));

  await dunningQueue.addBulk(jobPayloads);

  await job.updateProgress(100);

  console.log(`[dunning-scan] âœ… Enqueued ${dueEmails.length} dunning email job(s)`);
}

// â”€â”€â”€ Dunning Email Processor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function processDunningEmail(job: Job<DunningEmailJobData>): Promise<void> {
  const { failedPaymentId, templateId, sequenceOrder } = job.data;

  console.log(
    `[dunning-email] ğŸ“§ Sending dunning seq=${sequenceOrder} for payment ${failedPaymentId}`
  );

  await job.updateProgress(20);

  const result = await executeDunningEmail({ failedPaymentId, templateId, sequenceOrder });

  await job.updateProgress(100);

  if (result.success) {
    if (result.resendMessageId) {
      console.log(
        `[dunning-email] âœ… Sent: ${failedPaymentId} (msgId=${result.resendMessageId})`
      );
    } else {
      // Payment recovered/cancelled â€” email intentionally skipped
      console.log(`[dunning-email] â­ï¸  Skipped (payment no longer active): ${failedPaymentId}`);
    }
  } else {
    throw new Error(result.error ?? "Unknown dunning error");
  }
}

// â”€â”€â”€ Worker Factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export function createRetryWorker(): Worker {
  const worker = new Worker<RetryJobData | BatchScanJobData>(
    RETRY_QUEUE_NAME,
    async (job: Job) => {
      if (job.name === "batch-scan") {
        await processBatchScan(job as Job<BatchScanJobData>);
      } else if (job.name === "retry-job") {
        await processRetryJob(job as Job<RetryJobData>);
      } else {
        console.warn(`[worker] Unknown job type: ${job.name}`);
      }
    },
    {
      connection,
      concurrency: CONCURRENCY,
      autorun: false,
    }
  );

  worker.on("completed", (job: Job) => {
    console.log(`[worker] âœ… Completed: ${job.name}#${job.id}`);
  });

  worker.on("failed", (job: Job | undefined, err: Error) => {
    console.error(
      `[worker] âŒ Failed: ${job?.name ?? "unknown"}#${job?.id ?? "?"}`,
      err.message
    );
  });

  worker.on("error", (err: Error) => {
    console.error("[worker] Worker error:", err.message);
  });

  return worker;
}

// â”€â”€â”€ Dunning Worker Factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export function createDunningWorker(): Worker {
  const worker = new Worker<DunningEmailJobData | DunningScanJobData>(
    DUNNING_QUEUE_NAME,
    async (job: Job) => {
      if (job.name === "dunning-scan") {
        await processDunningScan(job as Job<DunningScanJobData>);
      } else if (job.name === "dunning-email") {
        await processDunningEmail(job as Job<DunningEmailJobData>);
      } else {
        console.warn(`[dunning-worker] Unknown job type: ${job.name}`);
      }
    },
    {
      connection,
      concurrency: CONCURRENCY,
      autorun: false,
    }
  );

  worker.on("completed", (job: Job) => {
    console.log(`[dunning-worker] âœ… Completed: ${job.name}#${job.id}`);
  });

  worker.on("failed", (job: Job | undefined, err: Error) => {
    console.error(
      `[dunning-worker] âŒ Failed: ${job?.name ?? "unknown"}#${job?.id ?? "?"}`,
      err.message
    );
  });

  worker.on("error", (err: Error) => {
    console.error("[dunning-worker] Worker error:", err.message);
  });

  return worker;
}
